KV方式のデータプール上にFSを構築する
FSはKVDBにトランザクション単位で書き込み
KVDBはスナップショット、整合性 バージョニング、 分散、回復を


分散時プライマリーノードが書き込み操作を支配する
KVDBはコレクション単位で
コレクションはそのIDのハッシュからノードのn%に割り振られ
コレクション中のオブジェクはそのkeyのハッシュからさらにn%に割り振られる
よって整合性管理のため
コレクションのプライマリーノードを作成し、
Keyごとのプライマリーノードも存在する

コレクションのプライマリーノードは
各ノードのKeyに対する書き込みの整合性を監督させる。
具体的にロックとCoW numberを発行

だから具体的にノードにあるトランザクションが来たら
プライマリーノードに編集範囲を伝えて
ロックを発行してもらって整合性を確保
Keyの各プライマリーノードに書き込み送信
終了後各Keyのプライマリーノードがキーを返却
全返却後、はじめにロックをおくってきたノードに返却

となる これですべての処理が正常に行われたかわかりやすく、 最小限のトラフィックになる

またいずれの処理でエラーが起きた場合ロールバック
エラーレスポンスとする

冗長化 レプリカはcollectionの範囲でkey単位で行われ、
keyのプライマリーノードが管理する


書き込みは追記方式
読み込みは最寄りレプリカ優先

回復について
Keyのプライマリーノードが死亡
検知としてコレクションのプライマリーノードからのハートビートとエラー
-> ハッシュリングの次のノードが受け継ぐ

コレクションのプライマリーノードが死亡
検知としてロックの取得ができないことをリングの次のノードに送信
そのノードが過半数のノードの報告を受け取った場合
-> ハッシュリングの次のノードが受け継ぐ
    全ノードに宣言

回復はKeyのプライマリーノードからCowNumberをもとに整合
整合破壊がおこっているかもしれないのでCowNumber - 1ですべき

## **1. 基本構造**

* **コレクション** = 1つの **仮想ドライブ（VD）**
* 各仮想ドライブは **ブロック** 単位で構成される
* **ブロック**はノード間で**分散管理**
* 仮想ドライブ内では \*\*クラスタ（ブロックの集合）\*\*を単位として扱い、その上に **FS風のツリー構造（VNode）** を構築

```
ノード
 └─ コレクション（仮想ドライブ）
     └─ ブロック（16MB単位）
         └─ クラスタ（ブロックの集合）
             └─ VNode（ファイル/ディレクトリ/文書ノード）
```

## **仮想ドライブ構造**
領域管理にはbitmapのカスタマイズ  
メタデータ  
データ部で構成  

各部分でファイル独立
```
collectionRUID/
    idvd
    .bitmap
    .meta
```

## **ブロックサイズ戦略**

### ● 課題

* **大きいブロック** → パディング多い（小ファイル非効率）
* **小さいブロック** → 通信爆発（大ファイルに不利）

### ● 解決策

- 複数のブロックサイズを用意する（例：1MB, 4MB, 16MB）
- クラスタが柔軟に複数ブロックを跨げる構造にする


## **クラスタ設計**

* クラスタは 1つ以上のブロックから構成される
* 各 `VNode` はクラスタのリストを持つ（→ 高速アクセス＆部分更新可能）


## **ファイルシステム（VNode）構造**

* inodeに似た構造でファイル・ディレクトリ・JSONノードを管理

```rust
struct VNode {
    id: VNodeId,                     // 一意なID（例えばUUID）
    name: String,                    // ファイル名やキー
    parent: Option<VNodeId>,        // 親ノードのID（ルートはNone）
    kind: NodeKind,                 // ファイル/ディレクトリ/JSONノードなど
    clusters: Vec<ClusterId>,       // 実データが格納されているクラスタID一覧
    children: Vec<VNodeId>,         // 子ノード（ディレクトリ/オブジェクト用）
    size: u64,                      // 合計サイズ
    created_at: u64,                // 作成時刻
    modified_at: u64,               // 更新時刻
}
```


## **分散ノード構成**

* 各ノードは複数ブロック（サイズ種別問わず）を保持
* **ブロックID → ノードマップ**を持ち、効率的なルーティングを実現
* 書き込みは**バージョン番号付きブロック**で安全に


## **パフォーマンス最適化策**

| 項目        | アプローチ例                   |
| --------- | ------------------------ |
| 小さなファイル対策 | SmallBlockに圧縮/パック配置      |
| 通信効率      | ブロック単位のstream転送          |
| バージョン管理   | Copy-on-Write / Snapshot |
| キャッシュ     | メタデータ, VNode, ブロックLRU    |



## ed
だから具体的には  
idvd内はブロック単位で分割されていて
1MBブロックは16MBブロックの16個をつかって出されている
allocはpow bitmap allocつかうのでパフォーマンスは気にならない

だから小さいリクエスト おもにメタデータの参照はネットワーク越しの参照が増えるけど
まぁノードが多かったら検索とかは

O(log_m N) m: ノード数

になるのでまぁ良くて
問題はただのデータ場所検索で VNをすべてたどる必要がある
cachingする場合、VN変更時にVNを逆走して全ノードのcacheを更新する必要がある

具体的に
- 更新がおこったら対象ノードへ行ってcacheを確認
- ヒットして更新が必要なら更新
- 接続がなくなった子ノードをcache(そのうち消える)

以上

- pathからファイルへのアクセスはかならずVNを順にたどる
- IDからファイルへのアクセスはかららずpathが通ってるかVNを逆走して調べる

idvd内で分散データをすべて表現する
root VN のcluster の RUID を含む idvdの先頭には root VN の cluster を置く
ツリーを構成するため

読み書きは